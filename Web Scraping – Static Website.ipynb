{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "daf440d2",
   "metadata": {},
   "source": [
    "# Web Scraping (Static Website)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "539b55f4",
   "metadata": {},
   "source": [
    "- **Static websites** are fixed and display the *same content for every user*, usually written exclusively in HTML and CSS. They don't provide any specialized content revision depending on user preference. \n",
    "\n",
    "\n",
    "- Internet communication involves a **server** and a **web browser**. To establish a connection between the two, a set of rules called **Hypertext Transfer Protocol (HTTP)** is used. Simply put, \n",
    "    1. The *web browser transmits an HTTP request to the server*\n",
    "    \n",
    "    2. The *server replies with an HTTP response along with the requested webpage in HTML*.\n",
    "\n",
    "\n",
    "- Static websites usually come with a *fixed number of pages that have a specific layout*. When the page runs on a browser, **the content is literally static and doesn’t change in response to user actions**. A static website is usually created with HTML and CSS in simple text editors like Notepad.\n",
    "\n",
    "\n",
    "- Even though the website will display the same thing with no intricate navigation details, static websites don’t need to feature just plain text. An HTML website can look beautiful, but the page’s **source code won’t change, no matter what actions a user takes on it.**\n",
    "\n",
    "\n",
    "- The following code examples are modification on top of [Web Scraping with Python - Beautiful Soup Crash Course](https://www.youtube.com/watch?v=XVv6mJpFOb0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "596c11b7",
   "metadata": {},
   "source": [
    "## Basic Functionality\n",
    "- The following code requests a **STATIC WEBSITE** that has different job postings that have the *company name*, *published date*, *skills needed*, and a reference to where to get *more information*.\n",
    "\n",
    "\n",
    "- The basic functionality of this code is to traverse over job postings and extract the necessary information. We're mostly interested in getting the latest job postings, so we filter the ones that have the `a few days ago` for the published date.\n",
    "\n",
    "- Important implementation points are\n",
    "\n",
    "    1. `html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=Python&txtLocation=').text`\n",
    "       - The `.text` tag is very important. If we don't specifically ask for the text, meaning that we ommit it, we'll get the **status code** of the request.\n",
    "       \n",
    "    2. `published_date = job.find('span', class_=\"sim-posted\").span.text.strip()`\n",
    "        - The `.span` is used here because the *published_date* is wrapped in two `<span></span>` tags. By just typing `.span`, we can shortcut the process and directly extract the text.\n",
    "\n",
    "    3. ` more_info = job.header.h2.a['href']`\n",
    "        - Similar to the previous implementation, here we do `.header` to get the header of the job container, then `.h2` to get into the `<h2></h2>` tag, and finally put `a['href']` to specifically get the **link** associated with the more information.\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ecafc2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=Python&txtLocation=').text\n",
    "\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "for job in jobs:\n",
    "    published_date = job.find('span', class_=\"sim-posted\").span.text.strip()\n",
    "    if 'few' in published_date:\n",
    "        company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '')\n",
    "        skills_needed = job.find('span', class_=\"srp-skills\").text.replace(' ', '')\n",
    "        # the link to the full-description of the job is under header-->h2-->a tag\n",
    "        # with the link being under the \"href\"\n",
    "        more_info = job.header.h2.a['href']\n",
    "\n",
    "        print(f\"Company Name: {company_name.strip()}\")\n",
    "        print(f\"Required Skills: {skills_needed.strip()}\")\n",
    "        print(f\"More Information: {more_info}\")\n",
    "\n",
    "        print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421b07d7",
   "metadata": {},
   "source": [
    "## Filtering Data\n",
    "- **Filtering Modification with Unfamiliar Skills**\n",
    "\n",
    "\n",
    "- The user is prompted to input the `unfamiliar_skills` that he has. These `unfamiliar_skills` are then scanned in each job description, only *jobs that don't have any of these `unfamiliar_skills`* are returned.\n",
    "\n",
    "\n",
    "- **Key Point:** The *set* implementation in Python is useful here. Specifically the following line provides us with this functionality\n",
    "    - `if(len(set(skills) & set(unfamiliar_skills)) == 0):`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f86d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "\n",
    "print('What are the skills you\\'re not familiar with?')\n",
    "print('When you\\'re done, press ENTER:')\n",
    "unfamiliar_skills = []\n",
    "skill = input('>>')\n",
    "while(skill != ''):\n",
    "    unfamiliar_skills.append(skill)\n",
    "    skill = input('>>')\n",
    "\n",
    "print(f'Filtering out {unfamiliar_skills} from job search')\n",
    "\n",
    "\n",
    "html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=Python&txtLocation=').text\n",
    "\n",
    "soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "for job in jobs:\n",
    "    published_date = job.find('span', class_=\"sim-posted\").span.text.strip()\n",
    "    if 'few' in published_date:\n",
    "        company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '').strip()\n",
    "        skills_needed = job.find('span', class_=\"srp-skills\").text.replace(' ', '').strip()\n",
    "\n",
    "        skills = skills_needed.split(',')\n",
    "        if(len(set(skills) & set(unfamiliar_skills)) == 0):\n",
    "\n",
    "        # the link to the full-description of the job is under header-->h2-->a tag\n",
    "        # with the link being under the \"href\"\n",
    "            more_info = job.header.h2.a['href']\n",
    "\n",
    "            print(f\"Company Name: {company_name}\")\n",
    "            print(f\"Required Skills: {skills_needed}\")\n",
    "            print(f\"More Information: {more_info}\")\n",
    "\n",
    "            print('')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63af0f24",
   "metadata": {},
   "source": [
    "## Automating \n",
    "- **Automation of Job Scraping**\n",
    "\n",
    "- After the user is prompted for filtering out `unfamiliar_skills`, the whole scraping project is automated to run with a **specified time intervals**. Here, this time is *30 minutes* and is done with a call to `time.sleep(sleep_time * 60)`. \n",
    "    1. After each scraping process, the scraped jobs are written to a file with the format `jobs_version[version]` inside the directory `jobs_pool`.\n",
    "```python\n",
    "with open(f'jobs_pool/jobs_version{version}.txt', 'w') as f:\n",
    "    find_jobs(f, version)\n",
    "# run every 30 minutes\n",
    "sleep_time = 30\n",
    "print(f\"Waiting for {sleep_time} minutes ...\")\n",
    "time.sleep(sleep_time * 60)\n",
    "```\n",
    "    2. The jobs are written into this `.txt` file with the format\n",
    "```python\n",
    "f.write(f\"Company Name: {company_name} \\n\")\n",
    "f.write(f\"Required Skills: {skills_needed} \\n\")\n",
    "f.write(f\"More Information: {more_info} \\n\")\n",
    "f.write('\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319d6f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "import time\n",
    "\n",
    "print('What are the skills you\\'re not familiar with?')\n",
    "print('When you\\'re done, press ENTER:')\n",
    "unfamiliar_skills = []\n",
    "skill = input('>>')\n",
    "while(skill != ''):\n",
    "    unfamiliar_skills.append(skill)\n",
    "    skill = input('>>')\n",
    "\n",
    "print(f'Filtering out {unfamiliar_skills} from job search')\n",
    "\n",
    "\n",
    "def find_jobs(f, version):\n",
    "    html_text = requests.get('https://www.timesjobs.com/candidate/job-search.html?searchType=personalizedSearch&from=submit&txtKeywords=Python&txtLocation=').text\n",
    "    soup = BeautifulSoup(html_text, 'lxml')\n",
    "\n",
    "    jobs = soup.find_all('li', class_=\"clearfix job-bx wht-shd-bx\")\n",
    "    for job in jobs:\n",
    "        published_date = job.find('span', class_=\"sim-posted\").span.text.strip()\n",
    "        if 'few' in published_date:\n",
    "            company_name = job.find('h3', class_=\"joblist-comp-name\").text.replace(' ', '').strip()\n",
    "            skills_needed = job.find('span', class_=\"srp-skills\").text.replace(' ', '').strip()\n",
    "\n",
    "            skills = skills_needed.split(',')\n",
    "            if(len(set(skills) & set(unfamiliar_skills)) == 0):\n",
    "\n",
    "                # the link to the full-description of the job is under header-->h2-->a tag\n",
    "                # with the link being under the \"href\"\n",
    "                more_info = job.header.h2.a['href']\n",
    "\n",
    "                # put the job information every 30 min in a .txt file\n",
    "                # formatted as jobs_version{version}.txt\n",
    "                f.write(f\"Company Name: {company_name} \\n\")\n",
    "                f.write(f\"Required Skills: {skills_needed} \\n\")\n",
    "                f.write(f\"More Information: {more_info} \\n\")\n",
    "                f.write('\\n')\n",
    "\n",
    "    print(f'File Saved: jobs_version{version}.txt')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    version = 1\n",
    "    while True:\n",
    "        with open(f'jobs_pool/jobs_version{version}.txt', 'w') as f:\n",
    "            find_jobs(f, version)\n",
    "        # run every 30 minutes\n",
    "        sleep_time = 30\n",
    "        print(f\"Waiting for {sleep_time} minutes ...\")\n",
    "        time.sleep(sleep_time * 60)\n",
    "        version += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
